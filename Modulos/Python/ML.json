[
    {
        "sklearn.linear_model.LogisticRegression()": " se usa para regression lineal, el problema debe ser binario y no estar desbalanceado el dataset, es sensitiva la ni\u00c3\u00b1a asi que hay que escalarla;solver='liblinear' para small datasets chama.", 
        "any_instance.fit(X_train,y_train)": "metodo que toma los datos para entrenar el modelo(general).", 
        "any_instance.predict(X_test)": "metodo que toma datos nuevos y hace predicciones a partir de haber sido entrenado(general).", 
        "sklearn.model_selection.train_test_split(X,y, test_size,random_state=)": "splitea las features y el target en 4 elementos(X_train,X_test,y_train,y_test) para trabajarlas en modelos; test_size= entre 0 y 1, haciendo el resto train_size.", 
        "sklearn.tree.DecisionTreeClassifier(random_state=0)": "un arbol de decision que se desempena mejor en classificacion;max_depth= profundidad del modelo, default infinito plp.", 
        "sklearn.metrics.accuracy_score(true_results,model_results)": "para calcular accuarcy, precision general lograda por el modelo.", 
        "sklearn.metrics.precision_score(true_results,model_results)": "para calcular precision, cant de  valores True de los True determinados por el modelo.", 
        "sklearn.metrics.recall_score(true_results,model_results)": "para calcular recall, cant de valores labeleados como True pero que son False(es decir, la clase rara 1).", 
        "sklearn.metrics.f1_score(true_results,model_results)": "para calcular f1-score, balance entre precision_score y recall_score.", 
        "sklearn.metrics.confusion_matrix(true_results,model_results)": "para calcular confusion-matrix que muestra los aciertos y fallos del modelo de la forma [[TP,FP],[FN,TN]].", 
        "sklearn.metrics.mean_squared_error(true_resuls,model_results)": "para calcular mean squared error.", 
        "sklearn.metrics.r2_score(true_resuls,model_results)": "para calcular r2_score.", 
        "sklearn.impute.SimpleImputer()": "para imputear de forma general en un df; strategy='mean'(default) se especifica con que llenar otra puede ser 'most frequent'; se usa con fit_transform().", 
        "sklearn.ensemble.RandomForestClassifier(n_estimators=)": "meta-estimator para controlar over-fitting, es un coleccion de arboles de decision; n_stimators=10(default);max_depth= infinito plp.", 
        "sklearn.preprocessing.OneHotEncoder()": "al igual que el pd.get_dummies se usa para conseguir dummies; inst.fit_transform(cols) para entranar y transformar las columnas; obj.toarray() para transformar lo que devuelva el metodo fit a un array; inst.get_feature_names_out() devuelve los nombres de como quedarian las columnas after transform.", 
        "sklearn.preprocessing.StandardScaler()": "para normalizar datos, este se usa cuando se sabe que el df tiene una distribucion normal; fit_transform() entrena al modelo y tansforma.", 
        "sklearn.model_selection.validation_curve(model, features, target, param_name=, param_range=)": "para testear un modelo con disintos parametros, devuelve una tupla de train_scores and test_scores con la cant de cv ;param_name= and param_range= parametro y rango de valores a testear; cv = cuantas partes dividir(default=5);scoring= metrica en str.", 
        "sklearn.svm.(SVC or SVR)": "vectores de soporte que se usan para identificar el mejor hyperplano para separar las variables; gamma= desde 0 hata 1; C[0:float(inf)].",
        "sklearn.model_selection.GridSearchCV(model,param_grid)": "para hyperparameters tunning; param_grid= dicc de forma {param: rango de parametros};cv= cuantos folds(default=5); obj.best_params devuelve los parametros con el que se obtuvo los mejores resultados;best_score_ devuelve el mejor score que se hizo.", 
        "sklearn.model_selection.RandomizedSearchCV(model, param_distributions)": "same as GridSearchCV pero este busca de forma random y es mas rapido pero a veces es mas bueno a veces no(gggg).",
        "sklearn.neighbors.KNeighborsClassifier()": "modelos de k-neighbors(knn), performan clasificacion utilizando distancia enre dos puntos en un plano, es sensible la ni\u00c3\u00b1a asi que hay que escalarla, papi cuidao con este caballo que se muere cuando son muchos datos pape;n_neighbors= : especifica k;p=[1,2];leaf_size=[1-50].", 
        "sklearn.preprocessing.MinMaxScaler()": "otra clase que se usa para normalizacion, al igual que standar scaler hay que fitearla y transorm.Cuando no se sabe si el df tiene distribucion normal.", 
        "sklearn.svm.(LinearSVC or LinearSVR)": "pape, hasta donde se esto es SVM con kernel=linear directamente, se usa cuando hay mas de 10mil datos que el otro se parte;parametros C=[1-inf];dual=False;epsilon tambiern configurables.", 
        "sklearn.preprocessing.LabelEncoder()": "es un transofrmador al igual que ohe pero labelea todo lo diferente en la columnas([1,2,3...]), se usa para targets columns, hay que fitear y transformar; inverse_transform(y) para volver los label a como estaban.", 
        "sklearn.ensemble.BaggingClassifier()": "su usa para para baggig gym....(risas)....entrena n numero de estimators y optimiza todo, es la clase general de RandomForest.", 
        "xgboost.(XGBClassifier or XGBRegressor)": "para crear un modelo de xgb, que es un gradientboost model pero especializado en trees que trata de evitar el overfitting; n_estimators= and max_depth= numeros enteros, entre mas mejor predice pero mayor chance de overfitearse; learning_rate= and subsample= floats desde 0.1 hasta 1,entre mas peque\u00f1o mas lento aprende pero mejor generaliza; reg_lambda= and reg_alpha= [0-inf] para regularizacion y alta dimensionalidad;eval_metric= metrica(default='error').", 
        "sklearn.cluster.KMeans(n_cluster=)": "modelo para clustering(unsupervised), funciona parecido al knn;max_iter: veces que se reinicia el modelo;n_init: veces que reinicia el modelo empezando en un lugar distinto; obj.model.inertia_: entre mas peque\u00f1o sea el numero mas 'exacto' y menos varianza tienen los puntos del centroide; se usa con fit_transform().", 
        "sklearn.compose.ColumnTransformer(transformers=[])": "para aplicar transformaciones a las columnas; transformers= las transformaciones en forma de tupla (str,transformador,columnas a seleccionar).", 
        "skelarn.pipeline.Pipeline(steps=[])": "para cuando se quiere procesar varios modelos en unas sola linea de trabajo; steps= introducimos los modelos en forma de tupla(str,modelo).", 
        "sklearn.metrics.mean_absolute_error()": "para calcular el MAE(risas), entre menor sea mas eficiente el modelo.", 
        "sklearn.metrics.roc_auc_score(y_test,y_pred)": "se usa generalmente para dataset desbalanceados, la idea es que el algoritmo se esfuerce en perfeccionar la habilidad de encontrar la clase rara(1)."
    }
]