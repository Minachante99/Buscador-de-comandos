[{"sklearn.linear_model.LogisticRegression()": "llama a la clase LogisticRegression, esta clase representa un modelo de regresion lineal que se usa para clasificar valores, no se usa cuando no es binario el problema o esta desbalanceado el dataset, es sensitiva la ni\u00c3\u00b1a asi que hay que escalarla;solver='liblinear' para small datasets chama, entre otros.", "any_instance.fit(X_train,y_train)": "metodo que toma los datos para entrenar el modelo(general).", "any_instance.predict(X_test)": "metodo que toma datos nuevos y hace predicciones a partir de haber sido entrenado(general).", "sklearn.model_selection.train_test_split(*arrays, test_size,train_size,random_state=0)": "esta funcion nos permite splitear separar los datos para training y los datos para test de nuestro modelo, los size deben ser en floats de 0 hatsta 1(si no se especifica train_size rellena lo que le fale a test_size); devuelve una tupla de 4 elementos(X_train,X_test,y_train,y_test).", "sklearn.tree.DecisionTreeClassifier(random_state=0)": "llama la clase DecisionTreeClassifier, esta clase es para un arbol de decision;criterion= default es gini;  max_features= can be used to limit the number of features considered at each node;max_depth= profundidad del modelo, default infinito plp; feature_importances_ muestra segun el modelo la importancia que tiene cada variable, devuelve una lista de numeros entre 0 y 1 correspondiendo el orden de las features.", "sklearn.metrics.accuracy_score(true_results,model_results)": "para calcular accuarcy, precision general lograda por el modelo.", "sklearn.metrics.precision_score(true_results,model_results)": "para calcular precision, porciento de valores realmente True de los True determinados por el modelo.", "sklearn.metrics.recall_score(true_results,model_results)": "para calcular recall, porciento de valores labeleados como True por el modelo de los True correctos.", "sklearn.metrics.f1_score(true_results,model_results)": "para calcular f1-score, balance entre precision_score y recall_score.", "sklearn.metrics.confusion_matrix(true_results,model_results)": " para calcular confusion-matrix que muestra los aciertos y fallos del modelo de la forma [[TP,FP],[FN,TN]].", "sklearn.metrics.mean_squared_error(true_resuls,model_results)": "para calcular mean squared error.", "sklearn.metrics.r2_score(true_resuls,model_results)": "para calcular r2_score.", "sklearn.impute.SimpleImputer()": "para imputear de forma general en un df para no tener que hacerlo manual por cada columna; strategy='mean'(default) se especifica con que llenar otra puede ser 'most frequen'; missing_values=np.nan(default) valores a llenar.Se le pasa fit() con los valores a aprender a rellenar y se usa transform() y se guarda en los valores a remplazar(simplificado en un solo paso con fit_transform())(mas en documentacion).", "sklearn.ensemble.RandomForestClassifier(n_estimators=)": "is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting, n_stimators=10(default).", "sklearn.preprocessing.OneHotEncoder()": "clase para transformar columnascuyo objetivo principal es hacer lo mismo que el pd.get_dummies pero ya devolver los valores en 0 y 1 para que puedan ser procesados; inst.fit_transform(rango) para entranar y transformar las columnas; toarray() para transformar lo que devuelva el metodo fit a un array; inst.get_feature_names_out() devuelve los nombres de como quedarian las columnas after transform.", "sklearn.preprocessing.StandardScaler()": "normalize the dataset to ensure that all features are on a similar scale. This step is crucial for logistic regression, as it helps prevent certain features from dominating the others in the model's learning process;fit_transform() entrena al modelo y tansforma.", "sklearn.model_selection.validation_curve(model, features, target, param_name=, param_range=)": "para testear un modelo con disintos parametros; model or inst del model;param_name= hyperparametro a tunniar(testear); param_range= rango  de valores a testear con el hyperparametro; cv = cuantas partes dividir(default=None);scoring= definir que scores pasar a cada fold; devuelve una tupla de train_scores and test_scores.", "sklearn.svm.(SVC or SVR)": "Support Vector Machine tanto para clasificacion o regresion vectores de soporte que se usan para identidficar el mejor hyperplano para separar las variables;parametro gamma desde 0 hata 1; C[0:float(inf)].", "sklearn.model_selection.GridSearchCV(model,param_grid)": "hyperparameters tunning,is a process that searches exhaustively through a manually specified subset of the hyperparameter space of the targeted algorithm; como param_grid debemos especificar en un dicc de forma {param: rango de parametros} los rangos para los hyper a tuniar,cv= cuantos folds(default=None); metodo fit con X_train and y_train; atributo best_params devuelve los parametros con el que se obtuvo los mejores resultados;best_score_ devuelve el mejor score que se hizo.", "sklearn.model_selection.RandomizedSearchCV(model, param_distributions)": "same as GridSearchCV pero este busca de forma random y es mas rapido pero no tan bueno.","sklearn.neighbors.KNeighborsClassifier()": "para crear modelos de k-neighbors(knn), que performan clasificacion segun los vecinos mas cercas a los datos de entrenamiento del modelo utilizando distancia enre dos puntos en un plano, es sensible la ni\u00c3\u00b1a asi que hay que escalarla;n_neighbors= : hyperparametro, especifica k.Papi cuidao con este caballo que se muere cuando son muchos datos pape;p=[1,2];leaf_size=[1-50].", "sklearn.preprocessing.MinMaxScaler()": "otra clase que se usa para normalizacion, al igual que standar scaler hay que fitearla y transorm. Usada en SVM, directamente con kernel linear.Hay que normalizar los datos antes de fitear y predecir.", "sklearn.svm.(LinearSVC or LinearSVR)": "pape, hasta donde se esto es SVM con kernel=linear directamente, se usa cuando hay mas de 10mil datos que el otro se parte;parametros C=[1-inf];dual=False,epsilon tambiern configurables.", "sklearn.preprocessing.LabelEncoder()": "es un transofrmador al igual que ohe pero labelea todo lo diferente en la columnas([1,2,3...]), se debe usar mas bien para target columns, hay que fitear y transformar, tiene un metodo inverse_transform(y) para volver los label a como estaban.", "sklearn.ensemble.BaggingClassifier()": "su usa para para baggig gym....(risas)....entrena n numero de estimators y optimiza todo, es la clase general de RandomForest.", "xgboost.(XGBClassifier or XGBRegressor)": "para crear un modelo de xgb, que es un gradientboost model pero especializado en trees que trata de evitar el overfitting; n_estimators= para cantidad de estimators, entre mas mejor predice pero mayor chance de overfitearse; max_depth= igual, mayor precision pero mas chance de overfitearse; learning_rate= entre mas peque\u00f1o mas lento aprende pero mejor generaliza; subsample(desde 0.1 hasta 1),reg_lambda[0-inf]: para regularizacion, reg_alpha[0-inf]: para alta dimensionalidad,eval_metric='auc'(default='error').", "sklearn.feature_selection.RFE(estimator,n_features_to_select)": "feature ranking with recursive feature elimination, dejando solo el # especificado de features que tienen mayor peso; n_features_to_select= features a dejar.", "sklearn.cluster.KMeans(n_cluster=)": "modelo para clustering(unsupervised), funciona parecido al knn;max_iter: veces que se reinicia el modelo;n_init: veces que reinicia el modelo empezando en un lugar distinto;model.inertia_: entre mas peque\u00f1o sea el numero mas 'exacto' y menos varianza tienen los puntos del centroide; se usa con fit_transform().", "sklearn.compose.ColumnTransformer(transformers=[])": "para aplicar transformaciones a las columnas; dentro de transformers debemos pasar las transformaciones en forma de tupla (str,transformador,columnas a seleccionar); make_column_selector(dtype_include=) para seleccionar las columnas desde adentro.", "skelarn.pipeline.Pipeline(steps=[])": "para cuando se quiere procesar varios modelos en unas sola linea de trabajo; en steps introducimos los modelos en forma de tupla(str,modelo).", "sklearn.metrics.mean_absolute_error()": "para calcular el MAE(risas), entre menor sea mas eficiente el modelo.", "sklearn.metrics.roc_auc_score(y_test,y_pred)": "se usa generalmente para dataset desbalanceados, la idea es que el algoritmo se esfuerce en perfeccionar la habilidad de encontrar la clase rara(1)."}]