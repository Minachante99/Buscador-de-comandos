[{"bs4.BeatifulSoup(arg,'parser')": "para parsear un documento con la clase, devuelve el documento parseado para acceder a la estructura; entre los parser tenemos html.parser y lxml(hay que instalarlo); los objetos de esta clase tienen atributos como .children and .descendants\u00c2\u00a0 que devuelve o solo los hijos o todo dentro del tag;.next_sibling or .previous_sibling para moverser entre cosas; .parent para ir a atr\u00c3\u00a1s a el tag parent..", "scrapy.Spider()": "clase para crear crawlers \"sencillos\", se debe definir funcion start_request donde se declaran los urls y debe retornar una lista de objetos scrapy.Request(url,callback); una funcion Parse que es la que va a llamar en el callback,esta es la que trabaja con las web. ", "bso.find_all(tag,attr)": "busca en todo el archivo lo que coincida, especificamos el tag y el atributo en forma de dicc; similar la func find() s\u00c3\u00b3lo encuentra el primero.", "bso.get_text()": "devuelve un string de lo que contenga quitando hiperv\u00c3\u00adnculos y todo.", "scrapy.spiders.CrawlSpider()": "clase para crawl, permite definir atributos de clase como allowed_domains - para que solo se mueva en esos dominios; start_urls - donde empezar;and rules donde pasamos la regla que queramos definir.", "scrapy.spiders.Rule(link_extractor)": "para definir de forma general como debe desarrollarse el spider,se debe definir funcion en callback; follow=true si no se definie callback,cb_kwargs acepta un dicc de argumentos a pasarle a la funcion en callback.", "scrapy.linkextractors.LinkExtractor()": "clase que permite buscar que links de todo tipo, en allow or deny podemos pasarle una regex.", "scrapy_func_parse(self,response)": "funcion que se debe definir en las clases de scrapy, solo parse en un obj Spider y parse_items en un CrawlSpider; selectores: .css(\"'tag'::text').extract_first() que se usa para seleccionar solo el texto del tag(sin los children) and .xpath que permite seleccionar texto de children.", "scrapy.Item()": "clase que scrapy usa alojar las cosa que se van parseando para procesarlas, creando los objetos de antemano para luego ser utilizada en el spider, cada objeto debe ser igual a scrapy.Field().", "request.get(arg)": "solicita \"arg\" a ese servidor y lo trata como un archivo local; headers= especificamos los headers; atributos .text and .json hace la conversion a ese tipo;cookies= para especificar cookies; auth= para especificar auth.\n \nurllib.request.urlretrieve(link,nombre_del_archivo)- descarga el archivo y le da nombre.", "urllb.error.HTTPError()": "para manejar errores que devuelve el server.", "requests.post(url,params)": "para loggins sencillos a una pagina, se busca el archivo y se ve que pide y se le pasa en parametros; si es una sola pagina lo que se va a visiar se puede usar las cookies de esta respuesta con obj.get_dict(). ", "request.Session()": "crea un objeto con el que modemos pasarle metodos post and get pero que el solo va llevando el manejo de las cookies.", "request.auth.HTTPBasicAuth(user,pass)": "crea un objeto de autentificacion al que se le pasa al post para siios con autentificacion sencilla.", "selenium.webdriver.chrome.options.Options()": "crea un objeto al que se le pasa obj.add_argument('--headless=new') para poder levantar chrome en headless mode.", "selenium.webdriver.Chrome(options)": "para utilizar un headless browser(Chrome en este caso); web_driver_obj.get(page) - parsea la pagina;web_driver_obj.page_source devuelve todo el documento ya habiendo ejecutado el javascript para luego ser parseado por bs."}]