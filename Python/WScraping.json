[
  {
    "bs4.BeatifulSoup(arg,'parser')": "para parsear un documento con la clase, devuelve el documento parseado para acceder a la estructura; entre los parser tenemos html.parser y lxml(hay que instalarlo); los objetos de esta clase tienen atributos como .children and .descendants\u00c2\u00a0 que devuelve o solo los hijos o todo dentro del tag;.next_sibling or .previous_sibling para moverser entre cosas; .parent para ir a atr\u00c3\u00a1s a el tag parent;  bso.find_all(tag,attr):busca en todo el archivo lo que coincida, especificamos el tag y el atributo en forma de dicc; similar la func find() s\u00c3\u00b3lo encuentra el primero; bso.get_text(): devuelve un string de lo que contenga quitando hiperv\u00c3\u00adnculos y todo.",
    "scrapy.Spider()": "clase para crear crawlers \"sencillos\", como atributo de clase se le debe definir nombre y se le pueden definir allowed_domains y strat_urls; definimos una funcion parse(self,response) para procesar el scrapeo.",
    "scrapy.spiders.CrawlSpider()": "clase para crawl, permite definir atributos de clase como allowed_domains - para que solo se mueva en esos dominios; start_urls - donde empezar; rules donde pasamos la regla que queramos definir y un atributo name obligatorio; definimos la func parse_items(self,response) : para procesar lo escrapeado.",
    "scrapy.spiders.Rule(link_extractor)": "para definir de forma general como debe desarrollarse el spider,se debe definir funcion en callback; follow=true si no se definie callback,cb_kwargs acepta un dicc de argumentos a pasarle a la funcion en callback.",
    "scrapy.linkextractors.LinkExtractor()": "clase que permite buscar que links de todo tipo, en allow or deny podemos pasarle una regex.",
    "requests.get(arg)": "solicita \"arg\" a ese servidor y lo trata como un archivo local; headers= especificamos los headers; atributos .text and .json hace la conversion a ese tipo;cookies= para especificar cookies; auth= para especificar auth.\n \nurllib.request.urlretrieve(link,nombre_del_archivo)- descarga el archivo y le da nombre.",
    "requests.post(url,params)": "para loggins sencillos a una pagina, se busca el archivo y se ve que pide y se le pasa en parametros; si es una sola pagina lo que se va a visiar se puede usar las cookies de esta respuesta con obj.get_dict(). ",
    "requests.Session()": "crea un objeto con el que modemos pasarle metodos post and get pero que el solo va llevando el manejo de las cookies.",
    "request.auth.HTTPBasicAuth(user,pass)": "crea un objeto de autentificacion al que se le pasa al post para siios con autentificacion sencilla.",
    "selenium.webdriver.chrome.options.Options()": "para establecer opciones a navegador chrome; obj.add_argument('--headless=new') para poder levantar chrome en headless mode.",
    "selenium.webdriver.Chrome(options)": "para utilizar Chrome; obj.get(page) : parsea la pagina; obj.page_source : devuelve todo el documento ya habiendo ejecutado el javascript; obj.click() : para clickear botones; obj.save_screenshot(nombre) : hace un screenshot de la pantalla; obj.find_element(s)(metodo) : para localizar elementos.",
    "selenium.webdriver.common.by.By": "para definir con que estructura se quiere localizar un elemento(By.XPATH,xpath). ",
    "scrapy.selector.Selector(obj)": "siendo obj un documento web anteriormente gettiado permite procesarlo usando xpath.",
    "Scrapy.crawler.CrawlerProcess(settings)": "para utilizar scrapy en solo script, en settings(diccionario) definimos distintos par\u00e1metros como LOG_LEVEL- para cuanta guanajda devuelve scrapy(ERROR para que sea nada),DOWNLOAD_DELAY- tiempo entre requests(1-3);AUTOTHROTTLE_TARGET_CONCURRENCY - para cantidad de veces hitteando el sitio web al mismo tiempo(3);HTTPCACHE_ENABLED - para que guarde un registro y no haga un request al mismo sitio(True),REDIRECT_MAX_TIMES - para estableces cuando redirects seguir; obj.crawl(scraper) - para pasarle el scraper; obj.start()- inicializa el scraper. ",
    "selenium.webdriver.support.ui.WebDriverWait(driver,sec)": "para hacer esperas en selenium, en sec la cantidad de segundos maximos que puede esperar; wait_obj.until(condiciones) - para esperar a que pase algo. ",
    "selenium.webdriver.support.expected_conditions.presence_of_element_located(path)": "principalmente se usa dentro de las esperas como condici\u00f3n de que aparezca algo, en path ponemos el camino al objeto. "
  }
]